{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: import subreddit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 posts downloaded, oldest post:2020-04-07 14:38:14 - status code: 200, now waiting 1 seconds before next pull. Patience...\n",
      "100 posts downloaded, oldest post:2020-01-28 18:03:38 - status code: 200, now waiting 1 seconds before next pull. Patience...\n",
      "100 posts downloaded, oldest post:2019-12-27 14:02:47 - status code: 200, now waiting 1 seconds before next pull. Patience...\n",
      "100 posts downloaded, oldest post:2019-11-04 02:20:22 - status code: 200, now waiting 1 seconds before next pull. Patience...\n",
      "100 posts downloaded, oldest post:2019-09-29 11:52:17 - status code: 200, now waiting 1 seconds before next pull. Patience...\n",
      "100 posts downloaded, oldest post:2019-08-26 02:58:05 - status code: 200, now waiting 1 seconds before next pull. Patience...\n",
      "100 posts downloaded, oldest post:2019-07-25 10:04:50 - status code: 200, now waiting 1 seconds before next pull. Patience...\n",
      "100 posts downloaded, oldest post:2019-06-19 18:21:40 - status code: 200, now waiting 1 seconds before next pull. Patience...\n",
      "100 posts downloaded, oldest post:2019-05-02 10:25:10 - status code: 200, now waiting 1 seconds before next pull. Patience...\n",
      "100 posts downloaded, oldest post:2019-03-04 15:05:25 - status code: 200, now waiting 1 seconds before next pull. Patience...\n",
      "100 posts downloaded, oldest post:2018-12-26 09:41:13 - status code: 200, now waiting 1 seconds before next pull. Patience...\n",
      "100 posts downloaded, oldest post:2018-09-28 08:12:21 - status code: 200, now waiting 1 seconds before next pull. Patience...\n",
      "100 posts downloaded, oldest post:2018-05-24 14:29:38 - status code: 200, now waiting 1 seconds before next pull. Patience...\n",
      "100 posts downloaded, oldest post:2018-01-15 05:11:38 - status code: 200, now waiting 1 seconds before next pull. Patience...\n",
      "100 posts downloaded, oldest post:2017-09-13 11:12:32 - status code: 200, now waiting 1 seconds before next pull. Patience...\n",
      "100 posts downloaded, oldest post:2017-06-14 08:55:17 - status code: 200, now waiting 1 seconds before next pull. Patience...\n",
      "100 posts downloaded, oldest post:2017-04-18 23:57:03 - status code: 200, now waiting 1 seconds before next pull. Patience...\n",
      "100 posts downloaded, oldest post:2017-02-12 00:03:00 - status code: 200, now waiting 1 seconds before next pull. Patience...\n",
      "100 posts downloaded, oldest post:2016-12-13 10:08:51 - status code: 200, now waiting 1 seconds before next pull. Patience...\n",
      "100 posts downloaded, oldest post:2016-09-21 13:39:32 - status code: 200, now waiting 1 seconds before next pull. Patience...\n",
      "\u001b[92m Does the imported dataframe match the request? False\u001b[00m\n",
      "Final DataFrame shape: (1975, 89), there are 0 duplicates\n",
      "100 posts downloaded, oldest post:2019-10-10 02:49:50 - status code: 200, now waiting 1 seconds before next pull. Patience...\n",
      "100 posts downloaded, oldest post:2019-07-27 03:17:49 - status code: 200, now waiting 1 seconds before next pull. Patience...\n",
      "100 posts downloaded, oldest post:2019-05-31 08:38:42 - status code: 200, now waiting 1 seconds before next pull. Patience...\n",
      "100 posts downloaded, oldest post:2019-02-01 00:23:05 - status code: 200, now waiting 1 seconds before next pull. Patience...\n",
      "100 posts downloaded, oldest post:2018-03-15 03:44:04 - status code: 200, now waiting 1 seconds before next pull. Patience...\n",
      "100 posts downloaded, oldest post:2016-10-01 15:31:20 - status code: 200, now waiting 1 seconds before next pull. Patience...\n",
      "\u001b[92m Does the imported dataframe match the request? False\u001b[00m\n",
      "Final DataFrame shape: (571, 82), there are 0 duplicates\n"
     ]
    }
   ],
   "source": [
    "def import_data_subreddit(latest_date = '1 July, 2020'):\n",
    "    \"\"\"\n",
    "    docstring: Puling the subreddit data from \"GlobalWarming\" and \"ConspiracyTheory\"\n",
    "    and save to /datasets folder.\n",
    "    param: latest_date to pull data from\n",
    "    \"\"\"\n",
    "    #imports\n",
    "    import sys\n",
    "    sys.path.insert(1, '../assets')\n",
    "    import requests\n",
    "    import pandas as pd\n",
    "    import time\n",
    "    from get_reddit_posts import get_reddit_posts\n",
    "\n",
    "    #############################################\n",
    "    #Pulling the Global Warming subreddit dataset\n",
    "\n",
    "\n",
    "    par = {\"subreddit\": \"GlobalWarming\", #The subreddit title\n",
    "           \"post_num\": 2000, # Numer of posts to pull from\n",
    "            \"time_1\": int(time.mktime(time.strptime(latest_date, '%d %B, %Y'))), # The latest pull time\n",
    "           \"API_limit\": 100, # API pull number limits for reddit per time\n",
    "           \"API_wait\": 1 #API wait time berfore the next pull\n",
    "          }\n",
    "\n",
    "\n",
    "\n",
    "    climate_change = get_reddit_posts(par[\"subreddit\"], par[\"post_num\"], par[\"time_1\"], par[ \"API_limit\"], par[\"API_wait\"])\n",
    "\n",
    "    file_path = \"../datasets/\" + par[\"subreddit\"] + \"_raw\" + \".csv\"\n",
    "    climate_change.to_csv(file_path)\n",
    "    \n",
    "    time.sleep(par[\"API_limit\"]) #Wait for a minute\n",
    "\n",
    "\n",
    "    ######################################\n",
    "    #Pulling the Conspiracy theory dataset\n",
    "\n",
    "    par = {\"subreddit\": \"ConspiracyTheory\", #The subreddit title\n",
    "           \"post_num\": 600, # Number of posts to pull from (limited reddit)\n",
    "            \"time_1\": int(time.mktime(time.strptime(latest_date, '%d %B, %Y'))), # The latest pull time\n",
    "           \"API_limit\": 100, # API pull number limits for reddit per time\n",
    "           \"API_wait\": 1 #API wait time berfore the next pull\n",
    "          }\n",
    "\n",
    "\n",
    "    cons_theory = get_reddit_posts(par[\"subreddit\"], par[\"post_num\"], par[\"time_1\"], par[ \"API_limit\"], par[\"API_wait\"])\n",
    "\n",
    "    file_path = \"../datasets/\" + par[\"subreddit\"] + \"_raw\" + \".csv\"\n",
    "    cons_theory.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: clean the dataset (major cleaning are on NLP end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data_subreddit():\n",
    "    \"\"\"\n",
    "    docstring: Cleaning the imported data and pickling the cleaned dataframe\n",
    "    \"\"\"\n",
    "    #imports\n",
    "    import pandas as pd\n",
    "    import regex as re\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    from nltk.corpus import stopwords # Import the stopword list\n",
    "\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "    import pickle\n",
    "\n",
    "    ####################Global warming#####\n",
    "\n",
    "    file_path = \"../datasets/\" + \"GlobalWarming\" + \"_raw\" + \".csv\"\n",
    "    df_gw = pd.read_csv(file_path)\n",
    "\n",
    "    # Keeping only a few columns which will be helpful during analysis\n",
    "    to_keep_clmns = ['author', 'created_utc', 'domain', 'id', 'num_comments', 'over_18',\n",
    "           'post_hint', 'score', 'selftext',\n",
    "           'title']\n",
    "\n",
    "    df_gw_clean = df_gw[to_keep_clmns]\n",
    "\n",
    "    #For title and selftext columns, I filled them with \" \" as they will be striped later, so I can merge them later.\n",
    "\n",
    "    df_gw_clean[\"title\"].fillna(\" \", inplace=True)\n",
    "    df_gw_clean[\"selftext\"].fillna(\" \", inplace=True)\n",
    "\n",
    "    #Merging the title and selftext for further processing\n",
    "\n",
    "    df_gw_clean['text_merged'] = df_gw_clean['title'] + \" \" + df_gw_clean['selftext']\n",
    "    df_gw_clean.drop(columns = [\"title\", \"selftext\"], inplace=True)\n",
    "\n",
    "    #For post_hint, I imputed them with \"Empty\"\n",
    "    df_gw_clean['post_hint'].fillna(\"Empty\", inplace=True)\n",
    "\n",
    "    ##################### ConspiracyTheory#######\n",
    "\n",
    "    file_path = \"../datasets/\" + \"ConspiracyTheory\" + \"_raw\" + \".csv\"\n",
    "    df_ct = pd.read_csv(file_path)\n",
    "\n",
    "    # Keeping only a few columns which will be helpful during analysis\n",
    "    df_ct_clean = df_ct[to_keep_clmns]\n",
    "\n",
    "\n",
    "    #For title and selftext columns, I filled them with \" \" as they will be striped later, so I can merge them later.\n",
    "\n",
    "    df_ct_clean[\"title\"].fillna(\" \", inplace=True)\n",
    "    df_ct_clean[\"selftext\"].fillna(\" \", inplace=True)\n",
    "\n",
    "    #Merging the title and selftext for further processing\n",
    "\n",
    "    df_ct_clean['text_merged'] = df_ct_clean['title'] + \" \" + df_ct_clean['selftext']\n",
    "    df_ct_clean.drop(columns = [\"title\", \"selftext\"], inplace=True)\n",
    "\n",
    "    #For post_hint, I imputed them with \"Empty\"\n",
    "    df_ct_clean['post_hint'].fillna(\"Empty\", inplace=True)\n",
    "\n",
    "    #########################MErging the two columns\n",
    "\n",
    "    df_gw_clean[\"subreddit\"] = \"GlobalWarming\"\n",
    "    df_ct_clean[\"subreddit\"] = \"ConspiracyTheory\"\n",
    "\n",
    "    df_reddit = pd.concat([df_gw_clean, df_ct_clean], axis = 0, ignore_index=True)\n",
    "\n",
    "    #Lots of cleaning on text\n",
    "\n",
    "    def text_cleaning(item):\n",
    "\n",
    "        #Removing \"\\n\" characters\n",
    "        item = re.sub(\"\\n\", \" \", item)\n",
    "        #Removing the [removed] characters\n",
    "        item = item.replace(\"[removed]\", \" \")\n",
    "        # Use regular expressions to do a find-and-replace\n",
    "        item = re.sub(\"[^a-zA-Z]\", \" \", item)\n",
    "        #Making all characters lower case\n",
    "        item = item.lower()\n",
    "        #Replacing multiple spaces\n",
    "        item = \" \".join(item.split())\n",
    "        #Removing stopwords\n",
    "        stops = stopwords.words(\"english\")\n",
    "        words = [w for w in item.split() if w not in stops]#stops\n",
    "        # Instantiate object of class PorterStemmer and stemming.\n",
    "        p_stemmer = PorterStemmer()\n",
    "        words = [p_stemmer.stem(i) for i in words]\n",
    "        # Adding space to stitch the words together\n",
    "        words = \" \".join(list(words)) \n",
    "\n",
    "        return words\n",
    "\n",
    "    df_reddit[\"text_merged\"] = df_reddit[\"text_merged\"].apply(text_cleaning)\n",
    "\n",
    "    #Stemming\n",
    "\n",
    "    df_reddit.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Pickling the dataframe as they are large!\n",
    "\n",
    "    pickle.dump(df_reddit, open('../datasets/df_reddit.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Transform features and Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_nlp_process():    \n",
    "    #imports\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pylab as plt\n",
    "    import seaborn as sns\n",
    "    import eli5\n",
    "\n",
    "    import regex as re\n",
    "    import nltk\n",
    "    #nltk.download('vader_lexicon')\n",
    "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "    from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "    from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "\n",
    "\n",
    "    from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, VotingClassifier\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    import pickle\n",
    "\n",
    "\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "    ################# Pulling cleaned data\n",
    "\n",
    "    df_reddit = pickle.load(open('../datasets/df_reddit.pkl', 'rb'))\n",
    "\n",
    "    ####Defining the target variable\n",
    "    df_reddit['target'] = df_reddit['subreddit'].replace({\"GlobalWarming\": 1, \"ConspiracyTheory\": 0})\n",
    "\n",
    "    ############# Bootstrapped the under-represented class to balance the classes:\n",
    "    n_bts_sample = df_reddit[(df_reddit[\"subreddit\"]==\"GlobalWarming\")].shape[0] - df_reddit[(df_reddit[\"subreddit\"]==\"ConspiracyTheory\")].shape[0]\n",
    "\n",
    "    df_btsp = df_reddit[(df_reddit[\"subreddit\"]==\"ConspiracyTheory\")].sample(n = n_bts_sample, replace=True, random_state=101)\n",
    "\n",
    "    df_reddit_btsp = pd.concat([df_reddit, df_btsp])\n",
    "\n",
    "    df_reddit_btsp.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "    ################### Defining X and Y variables:\n",
    "    X = df_reddit_btsp['text_merged']\n",
    "    y = df_reddit_btsp['target']\n",
    "\n",
    "    #################### Train/test split:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                        y,\n",
    "                                                        test_size = 0.25,\n",
    "                                                        stratify=y,\n",
    "                                                        random_state = 101)\n",
    "\n",
    "    ############################ Vectorizing a text using countVectorizer()\n",
    "    my_stop_words = [\"dec\", \"global\", \"http\", \"www\", \"com\", \"conspiraci\", \"warm\", \"climat\", \"remov\", \"theori\", \"theactualshadow\", \"co\"]\n",
    "    list_stop_words = nltk.corpus.stopwords.words('english')\n",
    "    list_stop_words.extend(my_stop_words)\n",
    "\n",
    "    vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                                 #ngram_range=(1,2),\n",
    "                                 tokenizer = None,\n",
    "                                 preprocessor = None,\n",
    "                                 stop_words = list_stop_words,\n",
    "                                 max_features = 4000) \n",
    "\n",
    "\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def rf_score_on_test_dataset(rf, X_test, y_test):\n",
    "    \n",
    "    from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "    from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "    \n",
    "    preds = rf.predict(X_test)\n",
    "\n",
    "    # Save confusion matrix values\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()\n",
    "\n",
    "\n",
    "    Accuracy = round((tp + tn) / (tp + tn + fp + fn), 3)\n",
    "    Precision = round(tp / (tp + fp), 3)\n",
    "    Recall = round(tp / (tp + fn), 3)\n",
    "\n",
    "    return Accuracy, Precision, Recall\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ml_retrain_registry_random_forest_subreddit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "randomForest model (n_estimators=200, max_depth=None, min_samples_split=2):\n",
      "  Accuracy: 0.968\n",
      "  Precision: 0.987\n",
      "  Recall: 0.947\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from urllib.parse import urlparse\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.WARN)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def ml_retrain_registry_random_forest_subreddit(n_estimators, max_depth, min_samples_split):\n",
    "    \n",
    "    #Step 1: import subreddit data\n",
    "    import_data_subreddit(latest_date = '1 July, 2020')\n",
    "    \n",
    "    #Step 2: clean the dataset (major cleaning are on NLP end)\n",
    "    clean_data_subreddit()\n",
    "    \n",
    "    #Step 3: Transform features and Train/Test split\n",
    "    X_train, X_test, y_train, y_test = train_test_nlp_process()\n",
    "    \n",
    "    #Step 4: ML build and score\n",
    "    criterion='gini'\n",
    "    rf = RandomForestClassifier(n_estimators, criterion, max_depth, min_samples_split)\n",
    "    \n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    Accuracy, Precision, Recall = rf_score_on_test_dataset(rf, X_test, y_test)\n",
    "    \n",
    "    #Step 5: Register the parameters and metric on MLflow\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "\n",
    "        print(\"randomForest model (n_estimators=%s, max_depth=%s, min_samples_split=%s):\" % (n_estimators, max_depth, min_samples_split))\n",
    "        print(\"  Accuracy: %s\" % Accuracy)\n",
    "        print(\"  Precision: %s\" % Precision)\n",
    "        print(\"  Recall: %s\" % Recall)\n",
    "\n",
    "        mlflow.log_param(\"n_estimators\", n_estimators)\n",
    "        mlflow.log_param(\"max_depth\", max_depth)\n",
    "        mlflow.log_metric(\"min_samples_split\", min_samples_split)\n",
    "        mlflow.log_metric(\"Accuracy\", Accuracy)\n",
    "        mlflow.log_metric(\"Precision\", Precision)\n",
    "        mlflow.log_metric(\"Recall\", Recall)\n",
    "\n",
    "        tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "\n",
    "        # Model registry does not work with file store\n",
    "        if tracking_url_type_store != \"file\":\n",
    "\n",
    "            mlflow.sklearn.log_model(rf, \"model\", registered_model_name=\"randomForestSubReddit\")\n",
    "        else:\n",
    "            mlflow.sklearn.log_model(rf, \"model\")\n",
    "            \n",
    "    \n",
    "    \n",
    "    \n",
    "n_estimators = 200\n",
    "max_depth = None\n",
    "min_samples_split = 2\n",
    "\n",
    "ml_retrain_registry_random_forest_subreddit(n_estimators, max_depth, min_samples_split)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the MLflow UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlflow ui"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsi]",
   "language": "python",
   "name": "conda-env-dsi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
